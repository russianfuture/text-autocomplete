project:
  name: text-autocomplete
  description: Auto-completion text model training and evaluation with LSTM and distilgpt2
  paths:
    data_dir: /home/assistant/text-autocomplete/data
    raw_dataset: /home/assistant/text-autocomplete/data/raw_dataset.csv
    processed_dataset: /home/assistant/text-autocomplete/data/dataset_processed.csv
    train_data: /home/assistant/text-autocomplete/data/train.csv
    val_data: /home/assistant/text-autocomplete/data/val.csv
    test_data: /home/assistant/text-autocomplete/data/test.csv
    input_file: /home/assistant/text-autocomplete/data/tweets.txt
    model_weights: /home/assistant/text-autocomplete/models/model_lstm_weights.pth

environment:
  device: cuda  # or cpu if cuda unavailable

tokenizer:
  lstm_model_name: DeepPavlov/rubert-base-cased
  distilgpt2_model_name: distilgpt2

data_processing:
  clean_text_regex:
    urls: "http\\S+|www\\S+"
    mentions: "@\\w+"
    non_word_chars: "[^\\w\\sа-яё]"
    whitespace: "\\s+"
  encoding: utf-8

dataset:
  vocab_padding_idx: 0
  batch_size: 64
  train_val_test_split:
    val_size: 0.2
    test_size_within_val: 0.5
  max_seq_length: null  # to be set dynamically based on dataset

model:
  lstm:
    vocab_size: null  # to be set dynamically
    embedding_dim: 128
    hidden_dim: 256
    padding_idx: 0
  distilgpt2:
    use_sampling: true
    top_k: 50
    num_return_sequences: 1

training:
  epochs: 10
  optimizer:
    type: Adam
    params:
      lr: 0.001
  loss_function:
    type: CrossEntropyLoss
    ignore_index: 0
  logging:
    print_epoch_results: true

evaluation:
  metrics:
    - rouge1
    - rouge2
  rouge_scorer:
    use_stemmer: true
  validation_samples_limit: 50
  print_examples_limit: 5

generation:
  lstm:
    max_length: 20
  distilgpt2:
    max_length_factor: 1.33  # (approx) factor of input + reference length

output:
  plots:
    figsize: [10, 6]
    xlabel: Epoch
    ylabel: Loss
    title: Training and Validation Loss
    legend: true
  print_summary: true
