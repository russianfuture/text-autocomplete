{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление ссылок (http, https и www)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Удаление упоминаний @username\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Удаление эмодзи и нестандартных символов (оставляем буквы, цифры, пробелы)\n",
    "    text = re.sub(r'[^\\w\\sа-яё]', '', text, flags=re.UNICODE)\n",
    "    \n",
    "    # Замена нескольких пробелов на один\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_file(input_path, output_path):\n",
    "    # Читаем строки из текстового файла\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        texts = f.readlines()\n",
    "        \n",
    "    # Очищаем и токенизируем\n",
    "    processed_texts = [clean_and_tokenize(text) for text in texts]\n",
    "    \n",
    "    # Формируем DataFrame: каждый токенизированный текст объединим в строку через пробел\n",
    "    df = pd.DataFrame({\n",
    "        'processed_text': [' '.join(tokens) for tokens in processed_texts]\n",
    "    })\n",
    "    \n",
    "    # Сохраняем в CSV\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Пути к файлам\n",
    "input_file = 'data/tweets.txt'\n",
    "output_file = 'data/dataset_processed.csv'\n",
    "\n",
    "process_file(input_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
