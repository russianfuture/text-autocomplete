{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 23:47:05.263746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices found: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Проверка доступности GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU devices found: {gpus}\")\n",
    "else:\n",
    "    print(\"GPU not found. Running on CPU.\")\n",
    "\n",
    "# Создаем токенизатор (русский Rubert)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\sа-яё]', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def create_training_samples(tokens):\n",
    "    if len(tokens) < 2:\n",
    "        return None\n",
    "    X = tokens[:-1]\n",
    "    Y = tokens[1:]\n",
    "    return X, Y\n",
    "\n",
    "def process_file(input_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        texts = f.readlines()\n",
    "    samples = []\n",
    "    for text in texts:\n",
    "        tokens = clean_and_tokenize(text)\n",
    "        pair = create_training_samples(tokens)\n",
    "        if pair:\n",
    "            X, Y = pair\n",
    "            samples.append({'X': X, 'Y': Y})\n",
    "    return samples\n",
    "\n",
    "def tokens_to_indices(samples, vocab):\n",
    "    X_indices = []\n",
    "    Y_indices = []\n",
    "    for sample in samples:\n",
    "        x_idx = [vocab.get(token, 0) for token in sample['X']]\n",
    "        y_idx = [vocab.get(token, 0) for token in sample['Y']]\n",
    "        X_indices.append(x_idx)\n",
    "        Y_indices.append(y_idx)\n",
    "    return X_indices, Y_indices\n",
    "\n",
    "input_file = '/home/assistant/text-autocomplete/data/tweets.txt'\n",
    "\n",
    "samples = process_file(input_file)\n",
    "\n",
    "# Создаем словарь токенов\n",
    "all_tokens = [token for sample in samples for token in sample['X']] + [token for sample in samples for token in sample['Y']]\n",
    "vocab = {token: idx+1 for idx, token in enumerate(sorted(set(all_tokens)))}  # 0 - паддинг\n",
    "\n",
    "X_indices, Y_indices = tokens_to_indices(samples, vocab)\n",
    "max_len = max(len(x) for x in X_indices)\n",
    "X_padded = pad_sequences(X_indices, maxlen=max_len, padding='post')\n",
    "Y_padded = pad_sequences(Y_indices, maxlen=max_len, padding='post')\n",
    "\n",
    "num_classes = len(vocab) + 1\n",
    "Y_categorical = np.array([to_categorical(seq, num_classes=num_classes) for seq in Y_padded])\n",
    "\n",
    "# Сплит на train/val/test\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X_padded, Y_categorical, test_size=0.2, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "# Построение модели\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_classes, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Обучение\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=64)\n",
    "\n",
    "# Функция генерации текста с модели\n",
    "def generate_text(seed_text, max_length=20):\n",
    "    tokens = tokenizer.tokenize(seed_text.lower())\n",
    "    for _ in range(max_length):\n",
    "        x_idx = [vocab.get(token, 0) for token in tokens]\n",
    "        x_padded = pad_sequences([x_idx], maxlen=max_len, padding='post')\n",
    "        preds = model.predict(x_padded)[0, len(tokens) - 1]  # предсказание для последнего токена\n",
    "        next_idx = np.argmax(preds)\n",
    "        next_token = next((tok for tok, idx in vocab.items() if idx == next_idx), None)\n",
    "        if next_token is None or next_token == '[SEP]':\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "        if len(tokens) >= max_length:\n",
    "            break\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Пример генерации\n",
    "print(generate_text(\"Привет, как дела\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
